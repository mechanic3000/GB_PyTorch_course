{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f22ed424",
   "metadata": {},
   "source": [
    "### Урок 3. Dataset, Dataloader, BatchNorm, Dropout, Оптимизация\n",
    "\n",
    "    Создать Dataset для загрузки данных (sklearn.datasets.fetch_california_housing)\n",
    "    Обернуть его в Dataloader\n",
    "    Написать архитектуру сети, которая предсказывает стоимость недвижимости. Сеть должна включать BatchNorm слои и Dropout (или НЕ включать, но нужно обосновать)\n",
    "    \n",
    "    Сравните сходимость Adam, RMSProp и SGD, сделайте вывод по качеству работы модели\n",
    "\n",
    "    train-test разделение нужно сделать с помощью sklearn random_state=13, test_size = 0.25\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1424ed4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "import sklearn.datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "\n",
    "RANDOM_STATE = 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "da61c008",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CaliforniaHousingDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, train=True):\n",
    "        self.dataset = sklearn.datasets.fetch_california_housing(return_X_y=True)\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(self.dataset[0], self.dataset[1], \n",
    "                                                            random_state=RANDOM_STATE, test_size=0.25)\n",
    "        \n",
    "        self.suffix = '_train' if train else '_test'\n",
    "        self.X = torch.Tensor(eval(f'self.X{self.suffix}'))\n",
    "        self.y = torch.Tensor(eval(f'self.y{self.suffix}'))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx, :], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d38fbadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = CaliforniaHousingDataset()\n",
    "test_data = CaliforniaHousingDataset(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "69afe18c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15480, 5160)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data), len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "524e04e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_data,\n",
    "                                           batch_size=32,\n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_data,\n",
    "                                          batch_size=8,\n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "93287f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, activation=\"relu\"):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(input_dim, output_dim)\n",
    "        self.activation = activation\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        if self.activation == \"relu\":\n",
    "            return F.relu(x)\n",
    "        if self.activation == \"sigmoid\":\n",
    "            return F.sigmoid(x)\n",
    "        raise RuntimeError\n",
    "        \n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = Perceptron(input_dim, 4*hidden_dim)\n",
    "        self.dp = nn.Dropout(0.25)\n",
    "        self.bn = nn.BatchNorm1d(4*hidden_dim)\n",
    "        self.fc2 = Perceptron(4*hidden_dim, 2*hidden_dim)\n",
    "        self.fc3 = Perceptron(2*hidden_dim, 1, \"relu\")\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.dp(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "06cf20d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = FeedForward(train_loader.dataset.X_train.shape[1], 32)\n",
    "\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "1bab3d79",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train_function():\n",
    "\n",
    "    num_epochs = 25\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss, running_items = 0.0, 0.0\n",
    "        net.train()\n",
    "\n",
    "        for i, data in enumerate(train_loader):\n",
    "            inputs, labels = data[0], data[1]\n",
    "\n",
    "            # обнуляем градиент\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs.squeeze(), labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # выводим статистику о процессе обучения\n",
    "            running_loss += loss.item()\n",
    "            running_items += len(labels)\n",
    "\n",
    "            # выводим статистику о процессе обучения\n",
    "            if i % 200 == 0:    # печатаем каждые 200 шагов\n",
    "                net.eval()\n",
    "                test_running_loss, test_running_total = 0.0, 0.0\n",
    "\n",
    "                for j, data in enumerate(test_loader):\n",
    "\n",
    "                    test_outputs = net(data[0])\n",
    "                    test_running_total += len(data[1])\n",
    "                    test_loss = criterion(test_outputs.squeeze(), data[1]) \n",
    "                    test_running_loss += test_loss.item()\n",
    "\n",
    "                print(f'Epoch [{epoch + 1}/{num_epochs}]. ' \\\n",
    "                      f'Step [{i + 1}/{len(train_loader)}]. ' \\\n",
    "                      f'Loss: {running_loss / running_items:.3f}. '\\\n",
    "                      f'Test loss: {test_running_loss / test_running_total:.3f}')\n",
    "\n",
    "                running_loss, running_items = 0.0, 0.0\n",
    "\n",
    "\n",
    "    print('Training is finished!\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "7d2a6fb9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/25]. Step [1/484]. Loss: 0.040. Test loss: 0.057\n",
      "Epoch [1/25]. Step [201/484]. Loss: 0.014. Test loss: 0.057\n",
      "Epoch [1/25]. Step [401/484]. Loss: 0.014. Test loss: 0.059\n",
      "Epoch [2/25]. Step [1/484]. Loss: 0.032. Test loss: 0.062\n",
      "Epoch [2/25]. Step [201/484]. Loss: 0.014. Test loss: 0.055\n",
      "Epoch [2/25]. Step [401/484]. Loss: 0.014. Test loss: 0.059\n",
      "Epoch [3/25]. Step [1/484]. Loss: 0.034. Test loss: 0.059\n",
      "Epoch [3/25]. Step [201/484]. Loss: 0.016. Test loss: 0.056\n",
      "Epoch [3/25]. Step [401/484]. Loss: 0.014. Test loss: 0.055\n",
      "Epoch [4/25]. Step [1/484]. Loss: 0.039. Test loss: 0.056\n",
      "Epoch [4/25]. Step [201/484]. Loss: 0.013. Test loss: 0.056\n",
      "Epoch [4/25]. Step [401/484]. Loss: 0.014. Test loss: 0.070\n",
      "Epoch [5/25]. Step [1/484]. Loss: 0.046. Test loss: 0.055\n",
      "Epoch [5/25]. Step [201/484]. Loss: 0.014. Test loss: 0.056\n",
      "Epoch [5/25]. Step [401/484]. Loss: 0.014. Test loss: 0.066\n",
      "Epoch [6/25]. Step [1/484]. Loss: 0.048. Test loss: 0.069\n",
      "Epoch [6/25]. Step [201/484]. Loss: 0.014. Test loss: 0.055\n",
      "Epoch [6/25]. Step [401/484]. Loss: 0.014. Test loss: 0.058\n",
      "Epoch [7/25]. Step [1/484]. Loss: 0.044. Test loss: 0.055\n",
      "Epoch [7/25]. Step [201/484]. Loss: 0.014. Test loss: 0.056\n",
      "Epoch [7/25]. Step [401/484]. Loss: 0.013. Test loss: 0.056\n",
      "Epoch [8/25]. Step [1/484]. Loss: 0.050. Test loss: 0.054\n",
      "Epoch [8/25]. Step [201/484]. Loss: 0.014. Test loss: 0.088\n",
      "Epoch [8/25]. Step [401/484]. Loss: 0.014. Test loss: 0.054\n",
      "Epoch [9/25]. Step [1/484]. Loss: 0.040. Test loss: 0.060\n",
      "Epoch [9/25]. Step [201/484]. Loss: 0.013. Test loss: 0.065\n",
      "Epoch [9/25]. Step [401/484]. Loss: 0.014. Test loss: 0.060\n",
      "Epoch [10/25]. Step [1/484]. Loss: 0.032. Test loss: 0.054\n",
      "Epoch [10/25]. Step [201/484]. Loss: 0.014. Test loss: 0.054\n",
      "Epoch [10/25]. Step [401/484]. Loss: 0.013. Test loss: 0.057\n",
      "Epoch [11/25]. Step [1/484]. Loss: 0.049. Test loss: 0.054\n",
      "Epoch [11/25]. Step [201/484]. Loss: 0.013. Test loss: 0.055\n",
      "Epoch [11/25]. Step [401/484]. Loss: 0.014. Test loss: 0.055\n",
      "Epoch [12/25]. Step [1/484]. Loss: 0.051. Test loss: 0.059\n",
      "Epoch [12/25]. Step [201/484]. Loss: 0.014. Test loss: 0.056\n",
      "Epoch [12/25]. Step [401/484]. Loss: 0.014. Test loss: 0.054\n",
      "Epoch [13/25]. Step [1/484]. Loss: 0.039. Test loss: 0.055\n",
      "Epoch [13/25]. Step [201/484]. Loss: 0.014. Test loss: 0.060\n",
      "Epoch [13/25]. Step [401/484]. Loss: 0.013. Test loss: 0.059\n",
      "Epoch [14/25]. Step [1/484]. Loss: 0.037. Test loss: 0.056\n",
      "Epoch [14/25]. Step [201/484]. Loss: 0.013. Test loss: 0.064\n",
      "Epoch [14/25]. Step [401/484]. Loss: 0.014. Test loss: 0.054\n",
      "Epoch [15/25]. Step [1/484]. Loss: 0.036. Test loss: 0.061\n",
      "Epoch [15/25]. Step [201/484]. Loss: 0.014. Test loss: 0.055\n",
      "Epoch [15/25]. Step [401/484]. Loss: 0.013. Test loss: 0.054\n",
      "Epoch [16/25]. Step [1/484]. Loss: 0.037. Test loss: 0.069\n",
      "Epoch [16/25]. Step [201/484]. Loss: 0.014. Test loss: 0.068\n",
      "Epoch [16/25]. Step [401/484]. Loss: 0.014. Test loss: 0.059\n",
      "Epoch [17/25]. Step [1/484]. Loss: 0.046. Test loss: 0.055\n",
      "Epoch [17/25]. Step [201/484]. Loss: 0.013. Test loss: 0.056\n",
      "Epoch [17/25]. Step [401/484]. Loss: 0.013. Test loss: 0.059\n",
      "Epoch [18/25]. Step [1/484]. Loss: 0.041. Test loss: 0.054\n",
      "Epoch [18/25]. Step [201/484]. Loss: 0.013. Test loss: 0.055\n",
      "Epoch [18/25]. Step [401/484]. Loss: 0.014. Test loss: 0.055\n",
      "Epoch [19/25]. Step [1/484]. Loss: 0.040. Test loss: 0.053\n",
      "Epoch [19/25]. Step [201/484]. Loss: 0.013. Test loss: 0.053\n",
      "Epoch [19/25]. Step [401/484]. Loss: 0.013. Test loss: 0.054\n",
      "Epoch [20/25]. Step [1/484]. Loss: 0.019. Test loss: 0.071\n",
      "Epoch [20/25]. Step [201/484]. Loss: 0.014. Test loss: 0.059\n",
      "Epoch [20/25]. Step [401/484]. Loss: 0.013. Test loss: 0.056\n",
      "Epoch [21/25]. Step [1/484]. Loss: 0.047. Test loss: 0.054\n",
      "Epoch [21/25]. Step [201/484]. Loss: 0.013. Test loss: 0.056\n",
      "Epoch [21/25]. Step [401/484]. Loss: 0.013. Test loss: 0.053\n",
      "Epoch [22/25]. Step [1/484]. Loss: 0.046. Test loss: 0.053\n",
      "Epoch [22/25]. Step [201/484]. Loss: 0.014. Test loss: 0.054\n",
      "Epoch [22/25]. Step [401/484]. Loss: 0.013. Test loss: 0.054\n",
      "Epoch [23/25]. Step [1/484]. Loss: 0.058. Test loss: 0.052\n",
      "Epoch [23/25]. Step [201/484]. Loss: 0.014. Test loss: 0.053\n",
      "Epoch [23/25]. Step [401/484]. Loss: 0.013. Test loss: 0.068\n",
      "Epoch [24/25]. Step [1/484]. Loss: 0.047. Test loss: 0.054\n",
      "Epoch [24/25]. Step [201/484]. Loss: 0.013. Test loss: 0.057\n",
      "Epoch [24/25]. Step [401/484]. Loss: 0.013. Test loss: 0.055\n",
      "Epoch [25/25]. Step [1/484]. Loss: 0.036. Test loss: 0.055\n",
      "Epoch [25/25]. Step [201/484]. Loss: 0.013. Test loss: 0.054\n",
      "Epoch [25/25]. Step [401/484]. Loss: 0.013. Test loss: 0.060\n",
      "Training is finished!\n",
      "\n",
      "CPU times: user 1min 14s, sys: 1.2 s, total: 1min 15s\n",
      "Wall time: 40.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.001)\n",
    "train_function()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "cea98884",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/25]. Step [1/484]. Loss: 0.047. Test loss: 0.129\n",
      "Epoch [1/25]. Step [201/484]. Loss: 0.018. Test loss: 0.055\n",
      "Epoch [1/25]. Step [401/484]. Loss: 0.015. Test loss: 0.056\n",
      "Epoch [2/25]. Step [1/484]. Loss: 0.042. Test loss: 0.058\n",
      "Epoch [2/25]. Step [201/484]. Loss: 0.015. Test loss: 0.058\n",
      "Epoch [2/25]. Step [401/484]. Loss: 0.015. Test loss: 0.062\n",
      "Epoch [3/25]. Step [1/484]. Loss: 0.023. Test loss: 0.056\n",
      "Epoch [3/25]. Step [201/484]. Loss: 0.015. Test loss: 0.055\n",
      "Epoch [3/25]. Step [401/484]. Loss: 0.014. Test loss: 0.055\n",
      "Epoch [4/25]. Step [1/484]. Loss: 0.035. Test loss: 0.055\n",
      "Epoch [4/25]. Step [201/484]. Loss: 0.014. Test loss: 0.052\n",
      "Epoch [4/25]. Step [401/484]. Loss: 0.015. Test loss: 0.065\n",
      "Epoch [5/25]. Step [1/484]. Loss: 0.046. Test loss: 0.055\n",
      "Epoch [5/25]. Step [201/484]. Loss: 0.014. Test loss: 0.066\n",
      "Epoch [5/25]. Step [401/484]. Loss: 0.015. Test loss: 0.055\n",
      "Epoch [6/25]. Step [1/484]. Loss: 0.049. Test loss: 0.054\n",
      "Epoch [6/25]. Step [201/484]. Loss: 0.014. Test loss: 0.056\n",
      "Epoch [6/25]. Step [401/484]. Loss: 0.015. Test loss: 0.054\n",
      "Epoch [7/25]. Step [1/484]. Loss: 0.033. Test loss: 0.054\n",
      "Epoch [7/25]. Step [201/484]. Loss: 0.014. Test loss: 0.056\n",
      "Epoch [7/25]. Step [401/484]. Loss: 0.015. Test loss: 0.055\n",
      "Epoch [8/25]. Step [1/484]. Loss: 0.059. Test loss: 0.054\n",
      "Epoch [8/25]. Step [201/484]. Loss: 0.015. Test loss: 0.058\n",
      "Epoch [8/25]. Step [401/484]. Loss: 0.014. Test loss: 0.054\n",
      "Epoch [9/25]. Step [1/484]. Loss: 0.038. Test loss: 0.068\n",
      "Epoch [9/25]. Step [201/484]. Loss: 0.014. Test loss: 0.053\n",
      "Epoch [9/25]. Step [401/484]. Loss: 0.015. Test loss: 0.071\n",
      "Epoch [10/25]. Step [1/484]. Loss: 0.062. Test loss: 0.055\n",
      "Epoch [10/25]. Step [201/484]. Loss: 0.014. Test loss: 0.060\n",
      "Epoch [10/25]. Step [401/484]. Loss: 0.014. Test loss: 0.054\n",
      "Epoch [11/25]. Step [1/484]. Loss: 0.043. Test loss: 0.062\n",
      "Epoch [11/25]. Step [201/484]. Loss: 0.014. Test loss: 0.058\n",
      "Epoch [11/25]. Step [401/484]. Loss: 0.015. Test loss: 0.056\n",
      "Epoch [12/25]. Step [1/484]. Loss: 0.043. Test loss: 0.054\n",
      "Epoch [12/25]. Step [201/484]. Loss: 0.015. Test loss: 0.056\n",
      "Epoch [12/25]. Step [401/484]. Loss: 0.014. Test loss: 0.054\n",
      "Epoch [13/25]. Step [1/484]. Loss: 0.049. Test loss: 0.053\n",
      "Epoch [13/25]. Step [201/484]. Loss: 0.014. Test loss: 0.057\n",
      "Epoch [13/25]. Step [401/484]. Loss: 0.014. Test loss: 0.057\n",
      "Epoch [14/25]. Step [1/484]. Loss: 0.062. Test loss: 0.056\n",
      "Epoch [14/25]. Step [201/484]. Loss: 0.014. Test loss: 0.073\n",
      "Epoch [14/25]. Step [401/484]. Loss: 0.016. Test loss: 0.099\n",
      "Epoch [15/25]. Step [1/484]. Loss: 0.036. Test loss: 0.060\n",
      "Epoch [15/25]. Step [201/484]. Loss: 0.014. Test loss: 0.056\n",
      "Epoch [15/25]. Step [401/484]. Loss: 0.014. Test loss: 0.054\n",
      "Epoch [16/25]. Step [1/484]. Loss: 0.027. Test loss: 0.074\n",
      "Epoch [16/25]. Step [201/484]. Loss: 0.015. Test loss: 0.053\n",
      "Epoch [16/25]. Step [401/484]. Loss: 0.014. Test loss: 0.058\n",
      "Epoch [17/25]. Step [1/484]. Loss: 0.051. Test loss: 0.056\n",
      "Epoch [17/25]. Step [201/484]. Loss: 0.014. Test loss: 0.058\n",
      "Epoch [17/25]. Step [401/484]. Loss: 0.014. Test loss: 0.117\n",
      "Epoch [18/25]. Step [1/484]. Loss: 0.064. Test loss: 0.052\n",
      "Epoch [18/25]. Step [201/484]. Loss: 0.013. Test loss: 0.055\n",
      "Epoch [18/25]. Step [401/484]. Loss: 0.014. Test loss: 0.063\n",
      "Epoch [19/25]. Step [1/484]. Loss: 0.039. Test loss: 0.058\n",
      "Epoch [19/25]. Step [201/484]. Loss: 0.014. Test loss: 0.052\n",
      "Epoch [19/25]. Step [401/484]. Loss: 0.014. Test loss: 0.053\n",
      "Epoch [20/25]. Step [1/484]. Loss: 0.063. Test loss: 0.057\n",
      "Epoch [20/25]. Step [201/484]. Loss: 0.014. Test loss: 0.051\n",
      "Epoch [20/25]. Step [401/484]. Loss: 0.014. Test loss: 0.059\n",
      "Epoch [21/25]. Step [1/484]. Loss: 0.065. Test loss: 0.057\n",
      "Epoch [21/25]. Step [201/484]. Loss: 0.014. Test loss: 0.054\n",
      "Epoch [21/25]. Step [401/484]. Loss: 0.014. Test loss: 0.055\n",
      "Epoch [22/25]. Step [1/484]. Loss: 0.034. Test loss: 0.055\n",
      "Epoch [22/25]. Step [201/484]. Loss: 0.014. Test loss: 0.061\n",
      "Epoch [22/25]. Step [401/484]. Loss: 0.014. Test loss: 0.057\n",
      "Epoch [23/25]. Step [1/484]. Loss: 0.035. Test loss: 0.063\n",
      "Epoch [23/25]. Step [201/484]. Loss: 0.014. Test loss: 0.065\n",
      "Epoch [23/25]. Step [401/484]. Loss: 0.014. Test loss: 0.063\n",
      "Epoch [24/25]. Step [1/484]. Loss: 0.029. Test loss: 0.054\n",
      "Epoch [24/25]. Step [201/484]. Loss: 0.014. Test loss: 0.054\n",
      "Epoch [24/25]. Step [401/484]. Loss: 0.014. Test loss: 0.067\n",
      "Epoch [25/25]. Step [1/484]. Loss: 0.037. Test loss: 0.079\n",
      "Epoch [25/25]. Step [201/484]. Loss: 0.013. Test loss: 0.104\n",
      "Epoch [25/25]. Step [401/484]. Loss: 0.014. Test loss: 0.053\n",
      "Training is finished!\n",
      "\n",
      "CPU times: user 1min 7s, sys: 1.1 s, total: 1min 8s\n",
      "Wall time: 36.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "optimizer = torch.optim.RMSprop(net.parameters(), lr=0.001)\n",
    "train_function()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "243c3974",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/25]. Step [1/484]. Loss: 0.060. Test loss: 0.298\n",
      "Epoch [1/25]. Step [201/484]. Loss: 0.037. Test loss: 0.119\n",
      "Epoch [1/25]. Step [401/484]. Loss: 0.030. Test loss: 0.110\n",
      "Epoch [2/25]. Step [1/484]. Loss: 0.036. Test loss: 0.154\n",
      "Epoch [2/25]. Step [201/484]. Loss: 0.030. Test loss: 0.138\n",
      "Epoch [2/25]. Step [401/484]. Loss: 0.027. Test loss: 0.133\n",
      "Epoch [3/25]. Step [1/484]. Loss: 0.045. Test loss: 0.112\n",
      "Epoch [3/25]. Step [201/484]. Loss: 0.029. Test loss: 0.128\n",
      "Epoch [3/25]. Step [401/484]. Loss: 0.027. Test loss: 0.104\n",
      "Epoch [4/25]. Step [1/484]. Loss: 0.037. Test loss: 0.107\n",
      "Epoch [4/25]. Step [201/484]. Loss: 0.028. Test loss: 0.102\n",
      "Epoch [4/25]. Step [401/484]. Loss: 0.027. Test loss: 0.120\n",
      "Epoch [5/25]. Step [1/484]. Loss: 0.099. Test loss: 0.099\n",
      "Epoch [5/25]. Step [201/484]. Loss: 0.027. Test loss: 0.134\n",
      "Epoch [5/25]. Step [401/484]. Loss: 0.027. Test loss: 0.127\n",
      "Epoch [6/25]. Step [1/484]. Loss: 0.032. Test loss: 0.101\n",
      "Epoch [6/25]. Step [201/484]. Loss: 0.026. Test loss: 0.093\n",
      "Epoch [6/25]. Step [401/484]. Loss: 0.027. Test loss: 0.167\n",
      "Epoch [7/25]. Step [1/484]. Loss: 0.053. Test loss: 0.125\n",
      "Epoch [7/25]. Step [201/484]. Loss: 0.027. Test loss: 0.091\n",
      "Epoch [7/25]. Step [401/484]. Loss: 0.025. Test loss: 0.265\n",
      "Epoch [8/25]. Step [1/484]. Loss: 0.041. Test loss: 0.117\n",
      "Epoch [8/25]. Step [201/484]. Loss: 0.026. Test loss: 0.091\n",
      "Epoch [8/25]. Step [401/484]. Loss: 0.026. Test loss: 0.100\n",
      "Epoch [9/25]. Step [1/484]. Loss: 0.059. Test loss: 0.098\n",
      "Epoch [9/25]. Step [201/484]. Loss: 0.027. Test loss: 0.089\n",
      "Epoch [9/25]. Step [401/484]. Loss: 0.026. Test loss: 0.165\n",
      "Epoch [10/25]. Step [1/484]. Loss: 0.066. Test loss: 0.095\n",
      "Epoch [10/25]. Step [201/484]. Loss: 0.025. Test loss: 0.104\n",
      "Epoch [10/25]. Step [401/484]. Loss: 0.027. Test loss: 0.099\n",
      "Epoch [11/25]. Step [1/484]. Loss: 0.060. Test loss: 0.095\n",
      "Epoch [11/25]. Step [201/484]. Loss: 0.026. Test loss: 0.109\n",
      "Epoch [11/25]. Step [401/484]. Loss: 0.027. Test loss: 0.134\n",
      "Epoch [12/25]. Step [1/484]. Loss: 0.065. Test loss: 0.186\n",
      "Epoch [12/25]. Step [201/484]. Loss: 0.027. Test loss: 0.175\n",
      "Epoch [12/25]. Step [401/484]. Loss: 0.025. Test loss: 0.105\n",
      "Epoch [13/25]. Step [1/484]. Loss: 0.050. Test loss: 0.134\n",
      "Epoch [13/25]. Step [201/484]. Loss: 0.026. Test loss: 0.104\n",
      "Epoch [13/25]. Step [401/484]. Loss: 0.027. Test loss: 0.144\n",
      "Epoch [14/25]. Step [1/484]. Loss: 0.044. Test loss: 0.107\n",
      "Epoch [14/25]. Step [201/484]. Loss: 0.026. Test loss: 0.099\n",
      "Epoch [14/25]. Step [401/484]. Loss: 0.026. Test loss: 0.114\n",
      "Epoch [15/25]. Step [1/484]. Loss: 0.040. Test loss: 0.182\n",
      "Epoch [15/25]. Step [201/484]. Loss: 0.026. Test loss: 0.144\n",
      "Epoch [15/25]. Step [401/484]. Loss: 0.027. Test loss: 0.106\n",
      "Epoch [16/25]. Step [1/484]. Loss: 0.033. Test loss: 0.104\n",
      "Epoch [16/25]. Step [201/484]. Loss: 0.026. Test loss: 0.117\n",
      "Epoch [16/25]. Step [401/484]. Loss: 0.026. Test loss: 0.105\n",
      "Epoch [17/25]. Step [1/484]. Loss: 0.042. Test loss: 0.119\n",
      "Epoch [17/25]. Step [201/484]. Loss: 0.027. Test loss: 0.096\n",
      "Epoch [17/25]. Step [401/484]. Loss: 0.026. Test loss: 0.135\n",
      "Epoch [18/25]. Step [1/484]. Loss: 0.050. Test loss: 0.095\n",
      "Epoch [18/25]. Step [201/484]. Loss: 0.026. Test loss: 0.133\n",
      "Epoch [18/25]. Step [401/484]. Loss: 0.026. Test loss: 0.092\n",
      "Epoch [19/25]. Step [1/484]. Loss: 0.033. Test loss: 0.222\n",
      "Epoch [19/25]. Step [201/484]. Loss: 0.026. Test loss: 0.093\n",
      "Epoch [19/25]. Step [401/484]. Loss: 0.025. Test loss: 0.102\n",
      "Epoch [20/25]. Step [1/484]. Loss: 0.036. Test loss: 0.105\n",
      "Epoch [20/25]. Step [201/484]. Loss: 0.026. Test loss: 0.095\n",
      "Epoch [20/25]. Step [401/484]. Loss: 0.026. Test loss: 0.092\n",
      "Epoch [21/25]. Step [1/484]. Loss: 0.044. Test loss: 0.086\n",
      "Epoch [21/25]. Step [201/484]. Loss: 0.025. Test loss: 0.093\n",
      "Epoch [21/25]. Step [401/484]. Loss: 0.025. Test loss: 0.086\n",
      "Epoch [22/25]. Step [1/484]. Loss: 0.026. Test loss: 0.183\n",
      "Epoch [22/25]. Step [201/484]. Loss: 0.026. Test loss: 0.096\n",
      "Epoch [22/25]. Step [401/484]. Loss: 0.027. Test loss: 0.092\n",
      "Epoch [23/25]. Step [1/484]. Loss: 0.027. Test loss: 0.086\n",
      "Epoch [23/25]. Step [201/484]. Loss: 0.027. Test loss: 0.097\n",
      "Epoch [23/25]. Step [401/484]. Loss: 0.025. Test loss: 0.089\n",
      "Epoch [24/25]. Step [1/484]. Loss: 0.037. Test loss: 0.139\n",
      "Epoch [24/25]. Step [201/484]. Loss: 0.026. Test loss: 0.142\n",
      "Epoch [24/25]. Step [401/484]. Loss: 0.026. Test loss: 0.087\n",
      "Epoch [25/25]. Step [1/484]. Loss: 0.049. Test loss: 0.088\n",
      "Epoch [25/25]. Step [201/484]. Loss: 0.026. Test loss: 0.089\n",
      "Epoch [25/25]. Step [401/484]. Loss: 0.026. Test loss: 0.093\n",
      "Training is finished!\n",
      "\n",
      "CPU times: user 59.3 s, sys: 762 ms, total: 1min\n",
      "Wall time: 30.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.001)\n",
    "train_function()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee60b50",
   "metadata": {},
   "source": [
    "#### Вывод: \n",
    "    в данном примере лучший результат в совокупности оценка-время показал оптимизатор RMSprop. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_3_9",
   "language": "python",
   "name": "ml_3_9"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
